# SSSD SPECULATOR

This python package provides a fast implementation of a suffix-array based retrieval of tokens for speculative decoding.


## Installation

```
pip install -e .
```

If there are issues with C libraries try

```
conda install -c conda-forge gcc_linux-64 gxx_linux-64
```
or
```
conda install -c conda-forge libstdcxx-ng
```
## Usage

### Creating the large datastore

To create an index from tokenized data, you need to use a tokenizer to tokenize sentences or paragraphs, and provide them to the `Writer`:

```python
writer = speculator.Writer(
    index_file_path=datastore_path,
    vocab_size=128,
    max_chunk_size=12,
)

writer.add_entry([1, 2, 3, 4])
```

You can add chunks of texts (sentences/paragraphs) of at most 512 Mtokens to the datastore. We recommend not to provide a value to `max_chunk_size`,
and to use the tokenizer vocabulary size (after adding special tokens) as vocabulary size (**ALWAYS GIVE AT LEAST ONE MORE TOKEN COMPARED TO THE VOCAB SIZE, TO AVOID ERRORS**).
A larger integer than the real vocabulary size is also ok (but if you want to use less memory and don't need more than 16 bits to represent the
tokens don't make it larger than 2^16).

Once you are done adding text, call

```python
writer.finalize()
```

If you want to add text later you can do it simply by loading the writer again and specifying the same path where the current datastore is already stored. The file will be update in place. If you want to create a new index, first copy the old one and then use the copied one.

```python
writer = speculator.Writer(
    index_file_path=datastore_path,
    vocab_size=128,
    max_chunk_size=12,
)
```

Then the behavior is the same.

### Script to create datastores

You can use the file `datastore_creation/create_datastore.py` to create datastores for different model using different datasets.
In particular, you can find the methods to use ShareGPT, Ultrachat, a subset of the Pile, and a function to create a datastore
from your own tokenized data (sentences in a .npz file), that could be generated by a model.

If you want to generate data from the model to create an `.npz` file to then create a datastore, you can use the script `generate_data.py`
in the same folder. It only supports data from ShareGPT at the moment, please, add other sources in the `get_next_sample()` method.
Once you have this data, you can call `create_datastore_from_npz()` in `create_datastore.py` with the generated file. Remember to change
all the values accordingly.

**In general we reccomend using sharegpt, ultrachat and magpie for english datastores, which can be done by just
setting the model path and running the file.**

### Updating the live datastore

If you don't have a large datastore to use (you can set the path to ""), or if you have it but want to improve it with data
coming from the model, you can use the live datastore (see below). This runs in a separate thread and has no impact in the insertion
and retrieval of candidates.

### Getting drafts for speculation

You can speculate only based on the fixed datastore on the input, or on both.
First load, the reader:

```python
reader = speculator.Reader(index_file_path="/path/to/datastore",
                            stop_token=1,
                            max_search_entries=600,
                            prompt_branch_length=6,
                            prompt_prefix_length=2,
                            max_output_size=256,
                            live_datastore=True,
                            max_update_chunk_size=512 * 1024 * 1024,
                            max_indices=8,
                            update_interval_ms=20 * 60 * 1000,  # every 20 minutes
                            vocab_size=300_000
                            )
```

* `index_file_path`: the path from which to load the datastore (created with the Writer).
If you don't want to use a datastore, just provide an empty string to `index_file_path`.
* `stop_token` is typically the bos_token_id of the tokenizer, to split sentences in the datastore for limited improvements
in prediction quality. It is optional.
* `prompt_branch_length`: what is the maximum depth of the tries created from the prompt.
* `prompt_prefix_length`: how many tokens from the prefix to use to search in the prompt (input and optionally self_output).
* `max_output_size`: the maximum size of the generated sequence (optional, doesn't need to be correct).
* `live_datastore`: whether to update the datastore while running with the outputs generated by the model. The following parameters describe how to do it.
* `max_update_chunk_size`: Same as `max_chunk_size` in the Writer (how big the new chunks will be). If you are
using a datastore build with the Writer, this parameter does not need to be the same as the one used for building
the index (but there are no good reasons to have it different).
* `max_indices`: The maximum amount of indexes to keep in RAM. This is to keep retrieval time always constraint.
Letting the index size grow indefinitely does not improve prediction quality but increases retrieval time.
Optional (default 8, but if you are using smaller chunks, this should be higher! In general, the idea is to keep
RAM usage below 20 GB, which should be good and cheap enough).
* `update_interval_ms`: How often to update the datastore with the new data. Can be any number, but we suggest not
to go below the order of minutes, unless you are in specific settings and you know that the data might be reused
immediately.
* `vocab_size`: same as in Writer. Can be bigger than the actual size, but not smaller.
* `prompt_tokens_in_datastore`: the last `prompt_tokens_in_datastore` tokens of each prompt will be added to the live
datastore when it's updated, together with the self output.
* `max_topk`: maximum breadth of each node in the final verification trie. Needed for compatibility with SGLang
KV-cache allocation, not needed outside from SGLang.

#### How to use the input/self_output

To insert the input prompt you should call the method

```python
reader.put(input=[0, 24, 1, 2], seq_id=0)
```
Where `seq_id` is the sequence id.

To insert the self-output (the sequence which is being generated) you can add the tokens generated in the last forward with

```python
reader.stream_put(new_tokens=[0, 24, 1, 2], seq_id=0)
```

The `seq_id` is the index at the beginning, when the batch was full, also if the batch size decreases over time.

#### Getting the drafts

To pick the candidates from the datastore or the input/self output, call

```python
output_ids, decoding_masks = reader.get_candidates(
    prefixes=[[1, 2], [5, 6]],
    decoding_length=8,
    branch_length=3,
    seq_ids=[0,2]
)
```

This method will pick from all the sources available, so if there is a datastore it will use it, if there is a prompt it will use it,
if there is a self-output it will use it. If you don't want to use the datastore, provide an empty string at construction, if you don't
want to use the prompt input don't call the `put()` method, if you don't want to use the self-output don't use the `stream_put()` method.

`prefixes` are the tokens that you want to use to math a string to find its continuation in the datastore. It is a list of lists (even for batch size 1).
The batch indices should be the indices of the remaining sequences at the beginning of the generation, when the batch was full.
In the example above, it could be a case where the initial batch size was 4 ([0, ..., 3]), and sequences 1 and 3 were already terminated. 

The `output_ids` are a list of lists (ordered like the elements in the batch) while `decoding_masks` are a list of numpy bidimensional arrays.

When a sequence is finished, if you have used the `put`/`stream_put` methods you should call

```python
reader.finish_sequence(seq_id)
```

To clear the prompt/self-output cache, run:

```python
reader.finish_all()
```

### Asynchronicity

The `put` and `finish_sequence` methods run on separate threads. If you want them to be synchronous, use the `sync_put`
and `sync_finish_sequence` methods.

You can use the AsyncReader, which takes exactly the same parameters as the Reader, and makes also the `stream_put` method asycnhronous. This method is usually fast, and running it asynchronously requires additional synchronization
that counters the benefit. In general, there should be no need to use it (but you can test what is better).

## Additional details

For more details, you can check our paper https://arxiv.org/abs/2411.05894 . To cite our work:

```
@misc{marzollo2024sssdsimplyscalablespeculativedecoding,
      title={SSSD: Simply-Scalable Speculative Decoding}, 
      author={Michele Marzollo and Jiawei Zhuang and Niklas Roemer and Lorenz K. MÃ¼ller and Lukas Cavigelli},
      year={2024},
      eprint={2411.05894},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.05894}, 
}
```