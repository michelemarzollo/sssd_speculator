/*
 * Copyright (c) Huawei Technologies Co., Ltd. 2020-2020. All rights reserved.
 */
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/numpy.h>
#include <queue>
#include <reader.hpp>

namespace py = pybind11;

/* After how often to check if the update is un-paused (ms) */
static const int64_t g_noDatastoreUpdateSleepTime = 10;

/* Minimum amount of time (ms) to sleep after finishing updating the live datastore before starting the next update */
static const int MIN_SLEEP_TIME_MS = 1000;

/* The size of block pools of TrieNodes, for efficient memory pooling: we use multiple blocks of this size
to minimize synchronization between threads and improve efficiency within the single thread: each sequence will own
one or more of these block (locking is needed only to get blocks, not single TrieNodes). A block is implemented as
a vector, so that it is easy (for the only thread using it) to access it without synchronization. Making this number
bigger means using less blocks for longer sequences, but having more unused memory over time. Using a smaller number
allows more recycling (especially if only few sequences are very long). */
static const size_t MAX_POOL_BLOCK_SIZE = 2048;

/*
    indexFilePath:              The path from which the datastore should be loaded.
    stopToken:                  The token that corresponds to end of sentences in the datastore. Typically bos or eos.
                                    If not provided, no token will be used (with small performance decrease)
    maxSearchEntries:           When searching in the suffix array, this limits the number of possible continuations if
                                    the prefix has too many possible suffixes.
                                If there are multiple indices this number is divided by the number of indices to get
                                    how many entries are (roughly) used per index.
    promptBranchLength:         The maximum branch length of the continuations taken from the input and self-generated
                                    output.
    promptPrefixLength:         How many tokens should be used to search in the input and self-generated output
    maxOutPutSize:              The maximum tokens that will be generated (not necessary to be provided, just for
                                    performance).
    liveDatastoreUpdates:       Whether to use the outputs generated by the model to update the large datastore of text
    maxChunkSize:               The maximum size of the chunks of text, each one representing a separate suffix array.
                                Shouls always be 512 * 1024 * 1024, besides for testing reasons.
    maxIndexes:                 The maximum number of chunks of text to keep (to keep retrieval time low). Ideally <= 10
    updateIntervalMs:           How often to update the large datastore with the data generated by the model. Note that
                                    the updating process can take several minutes: It probably does not make sense to go
                                    below 10 * 60 * 1000.
    vocabSize:                  The size of the vocabulary of the LLM. This number can be greater than the real vocabulary
                                    size (with minor effiency penalties), but not smaller!
                                We add 100 in case some special tokens were added, but the vocabulary was not updated.
    maxBatchSize:               The maxium number of sequences that will be queried at the same time (will be adapted if
                                    too low).
    promptTokensInDatastore:    The last 'prompt_tokens_in_datastore' tokens from the input will be added together
                                    with the self output in the datastore. Defaults to 3.
    */
Reader::Reader(const std::string &indexFilePath, int stopToken, int maxSearchEntries, int promptBranchLength,
    int promptPrefixLength, int maxOutPutSize, bool liveDatastoreUpdates, int maxChunkSize, int maxIndexes,
    int updateIntervalMs, int vocabSize, int maxBatchSize, int promptTokensInDatastore)
    : stopToken(stopToken), promptCache(promptBranchLength, promptPrefixLength, promptTokensInDatastore),
      maxSearchEntries(maxSearchEntries), maxOutPutSize(maxOutPutSize), updateDatastore(liveDatastoreUpdates),
      maxChunkSize(maxChunkSize), maxIndexes(maxIndexes), updateIntervalMs(updateIntervalMs), stopThread(false),
      datastoreUpdatePause(false), maxBatchSize(maxBatchSize)
{
    logger = GetLogger();

    size_t pinnedThreads = GetPinnedCpuCount();
    size_t maxCpus = static_cast<size_t>(std::thread::hardware_concurrency());
    // There are also finish and update threads, logger, possible writer to disk
    size_t maxThreads = std::max(size_t(1), std::min(pinnedThreads, maxCpus) - 4);

    this->nodeBlockPool = std::make_shared<TrieNodeBlockPool>(MAX_POOL_BLOCK_SIZE);
    this->selfOutputPool = std::make_shared<VectorPool>(maxOutPutSize);

    SPDLOG_LOGGER_DEBUG(logger, "pinnedThreads: {}", pinnedThreads);
    SPDLOG_LOGGER_DEBUG(logger, "maxCpus: {}", maxCpus);
    SPDLOG_LOGGER_DEBUG(logger, "Thread pool size: {}", maxThreads);
    threadPool = std::make_shared<ThreadPool>(maxThreads);

    // Finish initializing the prompt cache properly (ugly, but hard to do otherwise)
    promptCache.SetPools(threadPool, nodeBlockPool, selfOutputPool);

    this->vocabSize = vocabSize + 100;
    this->indexes = std::deque<std::shared_ptr<SubIndex>>();

    // Allocate memory for Tries to be used during retrieval
    datastoreTriesToBuild.reserve(maxBatchSize);
    for (int i = 0; i < maxBatchSize; ++i) {
        datastoreTriesToBuild.emplace_back(stopToken, *nodeBlockPool);  // Construct each Trie in place
    }

    if (!indexFilePath.empty()) {
        if (!IsLittleEndian()) {
            throw std::runtime_error(
                "System is not little-endian. Please implement the reading logic (swap bytes) to read the index file.");
        }
        // File extension validation
        const std::string idxExtension = ".idx";
        if (indexFilePath.length() < idxExtension.size() ||
            indexFilePath.substr(indexFilePath.length() - idxExtension.size()) != idxExtension) {
            throw std::runtime_error(
                "Datastore index file must have a .idx extension: " + indexFilePath);
        }

        // Load the datastore
        std::ifstream index_file(indexFilePath, std::ios::binary);
        if (!index_file.is_open()) {
            throw std::runtime_error("Failed to open the file for reading.");
        }

        // Getting file length
        index_file.seekg(0, std::ios::end);
        size_t index_file_len = index_file.tellg();
        index_file.seekg(0, std::ios::beg);

        size_t bytes_read = 0;

        // Read the first 32 bits to determine token serialization format
        uint32_t first_flag;
        index_file.read(reinterpret_cast<char *>(&first_flag), sizeof(first_flag));
        bytes_read += sizeof(first_flag);

        size_t token_size = (first_flag == 0) ? 4 : 2;
        if (this->vocabSize > 65536 && token_size == 2) {
            // Print the values if they are different
            SPDLOG_LOGGER_WARN(logger,
                "Unexpected parameters: The vocabulary size is {} but the tokens in the datastore are represented with "
                "2 bytes. You are probably using a datastore generated with a different tokenizer. The datastore will "
                "be used anyways.",
                this->vocabSize);
        }

        if (first_flag != 0) {
            index_file.seekg(0, std::ios::beg);
            bytes_read = 0;
        }

        while (bytes_read < index_file_len) {
            // Read the text
            uint32_t data_file_len;
            index_file.read(reinterpret_cast<char *>(&data_file_len), sizeof(data_file_len));
            std::vector<char> data_u8(data_file_len);
            index_file.read(data_u8.data(), data_file_len);

            // Get the indices of the suffix array in the full file
            uint32_t suffixes_file_len;
            index_file.read(reinterpret_cast<char *>(&suffixes_file_len), sizeof(suffixes_file_len));
            std::vector<char> suffixes_bytes(suffixes_file_len);
            index_file.read(suffixes_bytes.data(), suffixes_file_len);

            bytes_read += sizeof(data_file_len) + sizeof(suffixes_file_len) + data_file_len + suffixes_file_len;

            this->indexes.push_back(std::make_shared<SubIndex>());
            SubIndex &subindex = *this->indexes.back();

            if (updateDatastore) {
                // Do this to avoid later memory deallocations to reserve more space
                // If the indexes loaded are much smaller than maxChunkSize this is a waste of RAM. So we suggest to
                // create properly sized indices to load in the beginning.
                subindex.data.reserve(maxChunkSize);
                subindex.indexData.reserve(maxChunkSize);
            }

            size_t num_elements_data = data_u8.size() / token_size;
            size_t num_elements_index = suffixes_bytes.size() / 4;

            if (num_elements_data != num_elements_index) {
                // Print the values if they are different
                SPDLOG_LOGGER_ERROR(logger,
                    "Data and index have different dimensions. Data size: {}, index size: {}",
                    num_elements_data,
                    num_elements_index);
                throw std::runtime_error(fmt::format(
                    "Index file is corrupted: Data size: {}, index size: {}", num_elements_data, num_elements_index));
            }

            subindex.data.resize(num_elements_data);
            subindex.indexData.resize(num_elements_index);

            if (token_size == 4) {
                const int *data_ptr = reinterpret_cast<const int *>(data_u8.data());
                for (size_t i = 0; i < num_elements_data; ++i) {
                    subindex.data[i] = data_ptr[i];
                }
            } else {
                const int16_t *data_ptr = reinterpret_cast<const int16_t *>(data_u8.data());
                for (size_t i = 0; i < num_elements_data; ++i) {
                    subindex.data[i] = static_cast<int>(data_ptr[i]);
                }
            }

            const int *suffixes_ptr = reinterpret_cast<const int *>(suffixes_bytes.data());
            for (size_t i = 0; i < num_elements_index; ++i) {
                subindex.indexData[i] = suffixes_ptr[i];
            }
        }

        // If there are more indexes in the file than maxIndexes remove the first ones
        while (this->indexes.size() > maxIndexes) {
            this->indexes.pop_front();
        }

        SPDLOG_LOGGER_DEBUG(logger, "Number of indexes: {}\n", this->indexes.size());

        this->hasDatastore = true;
        // Define how many elements per index to use for retrieval
        SetSearchEntriesPerIdx();
        
    } else {
        // No datastore, input only
        if (!liveDatastoreUpdates) {
            this->hasDatastore = false;
        }
    }
    if (liveDatastoreUpdates) {
        // Index used to avoid deallocations in the update thread
        newSubindex = std::make_shared<SubIndex>();
        newSubindex->data.reserve(maxChunkSize);
        newSubindex->indexData.reserve(maxChunkSize);
        this->hasDatastore = true;
        updateThread = std::thread(&Reader::UpdateIndexes, this);
    }
}

Reader::~Reader()
{
    // Signal the update thread to stop and wait for it to finish
    stopThread = true;
    if (updateThread.joinable()) {
        updateThread.join();
    }
}

void Reader::SearchCandidates(const std::vector<int32_t> &prefix,
    size_t branchLength,  // how many tokens per suffix to get to build the trie from each subindex
    Trie &trie)
{
    auto worker = [&](const SubIndex &subIndex) {

        ssize_t startOfIndices = -1;
        ssize_t endOfIndices = -1;

        // Binary search on the suffix array
        // Search for first element starting with same prefix
        int64_t leftAnchor = 0;
        int64_t rightAnchor = subIndex.indexData.size() - 1;
        while (leftAnchor <= rightAnchor) {
            int64_t middleAnchor = leftAnchor + (rightAnchor - leftAnchor) / 2;
            int32_t dataIndex = subIndex.indexData[middleAnchor];
            auto lineStart = subIndex.data.begin() + dataIndex;
            auto lineEnd = lineStart + prefix.size();

            auto cmpResult = compare_ranges(lineStart, lineEnd, prefix.begin(), prefix.end());

            if (cmpResult == CompareResult::EQUAL) {
                startOfIndices = middleAnchor;
                rightAnchor = middleAnchor - 1;
            } else if (cmpResult == CompareResult::LESS) {
                leftAnchor = middleAnchor + 1;
            } else {
                rightAnchor = middleAnchor - 1;
            }
        }

        if (startOfIndices == -1) {
            return;
        }

        leftAnchor = startOfIndices;
        rightAnchor = subIndex.indexData.size() - 1;
        while (leftAnchor <= rightAnchor) {
            int64_t middleAnchor = leftAnchor + (rightAnchor - leftAnchor) / 2;
            int32_t dataIndex = subIndex.indexData[middleAnchor];
            auto lineStart = subIndex.data.begin() + dataIndex;
            auto lineEnd = lineStart + prefix.size();

            auto cmpResult = compare_ranges(lineStart, lineEnd, prefix.begin(), prefix.end());

            if (cmpResult == CompareResult::EQUAL) {
                endOfIndices = middleAnchor;
                leftAnchor = middleAnchor + 1;
            } else if (cmpResult == CompareResult::LESS) {
                leftAnchor = middleAnchor + 1;
            } else {
                rightAnchor = middleAnchor - 1;
            }
        }

        std::unordered_set<size_t> matches_ranges;
        size_t indices_size = endOfIndices + 1 - startOfIndices;
        size_t initial_capacity = std::min(indices_size, subIndex.maxSearchEntries.load());

        std::vector<Slice> local_results;
        local_results.reserve(initial_capacity);

        double float_step = std::max(
            1.0, static_cast<double>(endOfIndices - startOfIndices) / static_cast<double>(subIndex.maxSearchEntries));
        double current_index = static_cast<double>(startOfIndices);

        while (current_index <= static_cast<double>(endOfIndices)) {
            size_t dataIndex = subIndex.indexData[static_cast<size_t>(std::round(current_index))];
            if (matches_ranges.insert(dataIndex).second) {
                // The start could be out of bounds, but since it is uncommon we add also illegal ranges (start >
                // end) and check later
                auto start = subIndex.data.begin() + dataIndex + prefix.size();
                auto end = std::min(start + branchLength, subIndex.data.end());
                local_results.emplace_back(start, end);
            }
            current_index += float_step;
        }

        for (const auto &res : local_results) {
            trie.InsertSlice(res);
        }
    };

    for (const std::shared_ptr<SubIndex> subIndexPtr : indexes) {
        worker(*subIndexPtr);
    }
}

std::pair<std::vector<int>, std::vector<int>> Reader::GetBatchElementCandidates(const std::vector<int> &prefix,
    int decoding_length, int branchLength, std::shared_ptr<bool[]> &maskBuffer, int seqId, Trie &datastoreTrie)
{
    std::priority_queue<ProbCandidateNode> pq;
    FinalTrie finalTrie(prefix.back(), decoding_length);
    // Add prompt information
    std::vector<std::pair<TrieNode *, int>> results;
    auto it = promptCache.sequences.find(seqId);
    if (it != promptCache.sequences.end()) {
        results = it->second->FindPrefix(prefix);
    }  // The sequence should be present, but in case it isn't for some reason, don't crash
    if (!results.empty()) {
        for (auto &res : results) {
            TrieNode *promptPrefix = res.first;
            int matchedPrefixSize = res.second;
            int totCount = promptPrefix->count;
            double discount_factor = 0.6 + 0.1 * matchedPrefixSize;
            for (const auto &p : promptPrefix->children) {
                double prob = static_cast<double>(p.second->count) / totCount;
                pq.emplace(p.first, p.second, 0.6 * prob, 1, finalTrie.GetRoot(), discount_factor);
            }
        }
    }

    bool trie_is_full = finalTrie.GetTotalNodes() >= decoding_length;
    size_t iter = 0;
    if (this->hasDatastore) {
        // Insert candidates from fixed datastore
        SearchCandidates({prefix.begin() + iter, prefix.end()}, branchLength, datastoreTrie);
        while (iter < prefix.size() - 1 &&
               datastoreTrie.GetRoot()->count < 50) {  // TODO: decide what to do here, the number is arbitrary
            iter++;
            SearchCandidates({prefix.begin() + iter, prefix.end()}, branchLength, datastoreTrie);
        }

        // insert first layer
        auto current_node = datastoreTrie.GetRoot();
        int totCount = current_node->count;
        for (const auto &p : current_node->children) {
            pq.emplace(p.first, p.second, static_cast<double>(p.second->count) / totCount, 1, finalTrie.GetRoot(), 1.0);
        }
    }

    // Pick all candidates from any source
    while (!pq.empty() && !trie_is_full) {
        auto top = pq.top();
        pq.pop();
        auto node_in_finalTrie = finalTrie.Insert(top.precedingNode, top.value);
        if (node_in_finalTrie == nullptr) {    // The parent node can't have more children
            continue;
        }

        trie_is_full = finalTrie.GetTotalNodes() >= decoding_length;

        if (!trie_is_full) {
            for (const auto &child : top.node->children) {
                pq.emplace(child.first,
                    child.second,
                    (static_cast<double>(child.second->count) / top.node->count) * top.prob * top.childDiscountFactor,
                    top.depth + 1,
                    node_in_finalTrie,
                    top.childDiscountFactor);
            }
        }
    }

    // Clear datastoreTrie for reusing it in the next call
    // false means "always clear the nodes, even if the blockPool is empty."
    datastoreTrie.Clear(false);

    auto res = finalTrie.GetCandidatesAndAttnMaskRaw(maskBuffer);

    return res;
}

std::tuple<std::vector<std::vector<int>>, std::vector<std::vector<int>>, std::vector<py::array_t<bool>>>
    Reader::GetCandidates(const std::vector<std::vector<int>> &prefixes, const std::vector<int> &decodingLengths,
        const std::vector<int> &branchLengths, const std::vector<int> &seqIds)
{
    std::lock_guard<std::mutex> lock(indexesMtx);
    size_t numPrefixes = prefixes.size();
    rawMasksBool.resize(numPrefixes);  // don't explicitely clear: you can parallelize even the clearing
    while (datastoreTriesToBuild.size() < numPrefixes) {
        datastoreTriesToBuild.emplace_back(stopToken, *nodeBlockPool);
    }

    futures.clear();
    futures.reserve(numPrefixes);
    for (size_t i = 0; i < numPrefixes; ++i) {
        // Enqueue each task and get future for the task result
        futures.push_back(
            threadPool->Enqueue(PRIORITY_HIGH, [this, i, &prefixes, &decodingLengths, &branchLengths, &seqIds]() {
                return this->GetBatchElementCandidates(prefixes[i],
                    decodingLengths[i],
                    branchLengths[i],
                    this->rawMasksBool[i],
                    seqIds[i],
                    datastoreTriesToBuild[i]);
            }));
    }

    candidates.clear();
    depths.clear();
    candidates.reserve(numPrefixes);
    depths.reserve(numPrefixes);

    for (auto &fut : futures) {
        auto res = fut.get();
        candidates.push_back(res.first);
        depths.push_back(res.second);
    }

    boolMasks.clear();
    boolMasks.resize(numPrefixes);

    // convert the boolMasks into numpy arrays
    for (size_t i = 0; i < numPrefixes; ++i) {
        size_t n = candidates[i].size() - 1;  // Does not include the last input element
        auto capsule = py::capsule(new std::shared_ptr<bool[]>(rawMasksBool[i]),
            [](void *ptr) { delete static_cast<std::shared_ptr<bool[]> *>(ptr); });
        boolMasks[i] = py::array_t<bool>({n, n}, rawMasksBool[i].get(), capsule);
    }

    return {candidates, depths, boolMasks};
}

std::tuple<std::vector<int>, std::vector<int>, std::vector<int>, std::vector<int>> Reader::GetBatchElementCandidatesSglang(
    const std::vector<int> &prefix, int decoding_length, int branchLength, int maxTopk, std::shared_ptr<bool[]> &maskBuffer,
    int seqId, Trie &datastoreTrie)
{
    std::priority_queue<ProbCandidateNode> pq;
    FinalTrie finalTrie(prefix.back(), decoding_length, maxTopk);
    // Add prompt information
    std::vector<std::pair<TrieNode *, int>> results;
    auto it = promptCache.sequences.find(seqId);
    if (it != promptCache.sequences.end()) {
        results = it->second->FindPrefix(prefix);
    }  // The sequence should be present, but in case it isn't for some reason, don't crash
    if (!results.empty()) {
        for (auto &res : results) {
            TrieNode *promptPrefix = res.first;
            int matchedPrefixSize = res.second;
            int totCount = promptPrefix->count;
            double discount_factor = 0.6 + 0.1 * matchedPrefixSize;
            for (const auto &p : promptPrefix->children) {
                double prob = static_cast<double>(p.second->count) / totCount;
                pq.emplace(p.first, p.second, 0.6 * prob, 1, finalTrie.GetRoot(), discount_factor);
            }
        }
    }

    bool trie_is_full = finalTrie.GetTotalNodes() >= decoding_length;
    size_t iter = 0;
    if (this->hasDatastore) {
        // Insert candidates from fixed datastore
        SearchCandidates({prefix.begin() + iter, prefix.end()}, branchLength, datastoreTrie);
        while (iter < prefix.size() - 1 &&
               datastoreTrie.GetRoot()->count < 50) {  // TODO: decide what to do here, the number is arbitrary
            iter++;
            SearchCandidates({prefix.begin() + iter, prefix.end()}, branchLength, datastoreTrie);
        }

        // insert first layer
        auto current_node = datastoreTrie.GetRoot();
        int totCount = current_node->count;
        for (const auto &p : current_node->children) {
            pq.emplace(p.first, p.second, static_cast<double>(p.second->count) / totCount, 1, finalTrie.GetRoot(), 1.0);
        }
    }

    // Pick all candidates from any source
    while (!pq.empty() && !trie_is_full) {
        auto top = pq.top();
        pq.pop();
        auto node_in_finalTrie = finalTrie.Insert(top.precedingNode, top.value);
        if (node_in_finalTrie == nullptr) {    // The parent node can't have more children
            continue;
        }

        trie_is_full = finalTrie.GetTotalNodes() >= decoding_length;

        if (!trie_is_full) {
            for (const auto &child : top.node->children) {
                pq.emplace(child.first,
                    child.second,
                    (static_cast<double>(child.second->count) / top.node->count) * top.prob * top.childDiscountFactor,
                    top.depth + 1,
                    node_in_finalTrie,
                    top.childDiscountFactor);
            }
        }
    }

    // Clear datastoreTrie for reusing it in the next call
    // false means "always clear the nodes, even if the blockPool is empty."
    datastoreTrie.Clear(false);

    auto res = finalTrie.GetCandidatesMaskSglang(maskBuffer);

    return res;
}


std::tuple<
        std::vector<std::vector<int>>,      // candidates
        std::vector<std::vector<int>>,      // depths
        std::vector<std::vector<int>>,      // firstChildIdx
        std::vector<std::vector<int>>,      // nextSiblingIdx
        std::vector<py::array_t<bool>>      // boolMasks
>
    Reader::GetCandidatesSglang(const std::vector<std::vector<int>> &prefixes, const std::vector<int> &decodingLengths,
        const std::vector<int> &branchLengths, const std::vector<int> &maxTopks, const std::vector<int> &seqIds)
{
    std::lock_guard<std::mutex> lock(indexesMtx);
    size_t numPrefixes = prefixes.size();
    rawMasksBool.resize(numPrefixes);  // don't explicitely clear: you can parallelize even the clearing
    while (datastoreTriesToBuild.size() < numPrefixes) {
        datastoreTriesToBuild.emplace_back(stopToken, *nodeBlockPool);
    }

    sglangFutures.clear();
    sglangFutures.reserve(numPrefixes);
    for (size_t i = 0; i < numPrefixes; ++i) {
        // Enqueue each task and get future for the task result
        sglangFutures.push_back(
            threadPool->Enqueue(PRIORITY_HIGH, [this, i, &prefixes, &decodingLengths, &branchLengths, &maxTopks, &seqIds]() {
                return this->GetBatchElementCandidatesSglang(prefixes[i],
                    decodingLengths[i],
                    branchLengths[i],
                    maxTopks[i],
                    this->rawMasksBool[i],
                    seqIds[i],
                    datastoreTriesToBuild[i]);
            }));
    }

    candidates.clear();
    depths.clear();
    firstChildren.clear();
    nextSiblings.clear();
    candidates.reserve(numPrefixes);
    depths.reserve(numPrefixes);
    firstChildren.reserve(numPrefixes);
    nextSiblings.reserve(numPrefixes);

    for (auto &fut : sglangFutures) {
        auto res = fut.get();
        candidates.push_back(std::move(std::get<0>(res)));
        depths.push_back(std::move(std::get<1>(res)));
        firstChildren.push_back(std::move(std::get<2>(res)));
        nextSiblings.push_back(std::move(std::get<3>(res)));
    }

    std::vector<py::array_t<bool>> boolMasks(numPrefixes);

    // convert the masks into numpy arrays
    for (size_t i = 0; i < numPrefixes; ++i) {
        size_t n = candidates[i].size();
        auto capsule = py::capsule(new std::shared_ptr<bool[]>(rawMasksBool[i]),
            [](void *ptr) { delete static_cast<std::shared_ptr<bool[]> *>(ptr); });
        boolMasks[i] = py::array_t<bool>({n, n}, rawMasksBool[i].get(), capsule);
    }

    return {candidates, depths, firstChildren, nextSiblings, boolMasks};
}

void Reader::Put(std::vector<int> &input, int seqId)
{
    this->promptCache.Put(input, seqId);
}

void Reader::AsyncPut(std::vector<int> &&input, int seqId)
{
    this->promptCache.AsyncPut(std::move(input), seqId);
}

void Reader::StreamPut(std::vector<int> &newTokens, int seqId)
{
    promptCache.StreamPut(newTokens, seqId);
}

void Reader::BatchedStreamPut(const std::vector<std::pair<std::vector<int>, int>> &newTokens)
{
    for (const auto &pair : newTokens) {
        promptCache.StreamPut(pair.first, pair.second);
    }
}

void Reader::FinishSequence(int seqId)
{
    if (updateDatastore && !datastoreUpdatePause) {
        std::vector<int32_t> sentence = promptCache.FinishSequenceAndGetOutput(seqId);
        {
            std::lock_guard<std::mutex> lock(newSentencesMtx);
            newSentences.push_back(std::move(sentence));
        }
    } else {
        promptCache.FinishSequence(seqId);
    }
}

void Reader::AsyncFinishSequence(int seqId)
{
    if (updateDatastore && !datastoreUpdatePause) {
        std::vector<int32_t> sentence = promptCache.AsyncFinishSequenceAndGetOutput(seqId);
        {
            std::lock_guard<std::mutex> lock(newSentencesMtx);
            newSentences.push_back(std::move(sentence));
        }
    } else {
        promptCache.AsyncFinishSequence(seqId);
    }
}

void Reader::FinishAll()
{
    this->promptCache.FinishAll();
}

void Reader::UpdateAttributes(
    int stopToken, int maxSearchEntries, int promptBranchLength, int promptPrefixLength, int maxOutPutSize, int inputTokensToPutInSelfOutput)
{
    this->stopToken = stopToken;

    // Define how many elements per index to use for retrieval
    this->maxSearchEntries = maxSearchEntries;
    SetSearchEntriesPerIdx();

    this->promptCache.SetParameters(promptBranchLength, promptPrefixLength, inputTokensToPutInSelfOutput);
}

void Reader::SetUpdatePauseFlag(bool flag)
{
    datastoreUpdatePause = flag;
}

/**
 * @brief Continuously collects generated outputs and updates suffix arrays at regular intervals.
 *
 * This method runs in parallel, periodically gathering the outputs produced thus far
 * and updating the suffix arrays with the latest information. To ensure retrieval
 * times remain low, it automatically removes older chunks from the once the total count exceeds `maxIndexes`.
 *
 * @note While the indexes are being updated, the reader keeps returning candidates based on the currently valid
 * indexes.
 */
void Reader::UpdateIndexes()
{
    auto start_time = std::chrono::steady_clock::now();
    auto end_time = std::chrono::steady_clock::now();
    int64_t elapsed_ms = 0;
    int counter = 0;

    while (!stopThread) {
        if (datastoreUpdatePause) {
            std::this_thread::sleep_for(std::chrono::milliseconds(g_noDatastoreUpdateSleepTime));
            continue;
        }
        // Adjust sleep time based on processing duration
        if (elapsed_ms < updateIntervalMs) {
            int sleep_ms = std::max<int>(updateIntervalMs - elapsed_ms, MIN_SLEEP_TIME_MS);
            int remaining_ms = sleep_ms;
            while (remaining_ms > 0 && !stopThread) {
                int sleep_time = std::min(remaining_ms, 1000);  // Sleep 1-second or less if remaining time is shorter
                std::this_thread::sleep_for(std::chrono::milliseconds(sleep_time));
                remaining_ms -= sleep_time;
            }
        } else {
            SPDLOG_LOGGER_DEBUG(logger,
                "The update interval ({} ms) is less than the time it took to update the index ({} ms)."
                "You might want to update the interval less frequently.",
                updateIntervalMs,
                elapsed_ms);
            std::this_thread::sleep_for(std::chrono::milliseconds(MIN_SLEEP_TIME_MS));
        }

        SPDLOG_LOGGER_DEBUG(logger, "Finished sleeping");

        start_time = std::chrono::steady_clock::now();

        // Copy new sentences and clear the original
        std::deque<std::vector<int>> sentencesToProcess;

        {
            std::lock_guard<std::mutex> lock(newSentencesMtx);
            sentencesToProcess.swap(newSentences);
        }

        if (!sentencesToProcess.empty()) {
            // Create (or clean-up if recycled) the new index (if recycled, make sure it is not in use anymore)
            while (newSubindex.use_count() > 1) {
                SPDLOG_LOGGER_WARN(logger,
                    "Someone is still using an index that was removed. If this warning persists there is likely an "
                    "implementation error");
                std::this_thread::sleep_for(std::chrono::milliseconds(2));
            }
            SPDLOG_LOGGER_DEBUG(logger, "Clearing the subindex");
            newSubindex->data.reserve(maxChunkSize);
            newSubindex->data.clear();
            newSubindex->indexData.reserve(maxChunkSize);
            newSubindex->indexData.clear();
            SPDLOG_LOGGER_DEBUG(logger, "Finished clearing the subindex");
            newSubindex->maxSearchEntries = 0;
            size_t currentChunkSize = indexes.empty() ? 0 : indexes.back()->data.size();
            bool substituteLastIndex = false;
            std::shared_ptr<SubIndex> toRemoveAndRecycle;  // defer destruction, and keep it in this thread
            if (!indexes.empty() && currentChunkSize < maxChunkSize) {
                // Resume last chunk, if present and not full
                toRemoveAndRecycle = indexes.back();
                SPDLOG_LOGGER_DEBUG(logger, "Copying data");
                newSubindex->data = toRemoveAndRecycle->data;
                newSubindex->maxSearchEntries = toRemoveAndRecycle->maxSearchEntries.load();
                SPDLOG_LOGGER_DEBUG(logger, "Finished copying data");
                substituteLastIndex = true;
            } else {
                toRemoveAndRecycle = std::make_shared<SubIndex>();
                toRemoveAndRecycle->data.reserve(maxChunkSize);
                toRemoveAndRecycle->indexData.reserve(maxChunkSize);
                currentChunkSize = 0;
            }

            SPDLOG_LOGGER_DEBUG(logger, "Appending sentences");
            for (std::vector<int> &sentence : sentencesToProcess) {
                if (sentence.size() > maxChunkSize) {
                    // Discard sentences that are too big (impossible, but just be safe)
                    continue;
                }
                if (currentChunkSize + sentence.size() > maxChunkSize) {
                    SPDLOG_LOGGER_DEBUG(logger, "Constructing  suffix array (didn't finish adding sentences)");
                    // Construct suffix array for the current chunk
                    int res = constructSuffixArrayInplace(newSubindex->data, newSubindex->indexData, vocabSize, logger);
                    if (res == 0) {
                        // suffix array construction successful
                        {
                            std::lock_guard<std::mutex> lock(indexesMtx);
                            if (substituteLastIndex) {
                                indexes.pop_back();  // remove the last index (the variable toRemoveAndRecycle keeps it
                                                     // for recycling)
                                substituteLastIndex = false;
                            }
                            indexes.push_back(std::move(newSubindex));
                            if (indexes.size() > maxIndexes) {
                                // This means we didn't remove an index in the end before appending one -> let's remove
                                // from front
                                toRemoveAndRecycle = indexes.front();
                                indexes.pop_front();
                            }
                        }
                    } else {
                        SPDLOG_LOGGER_WARN(logger,
                            "The construction of the suffix array returned error {}. The new data will be discarded.",
                            res);
                        if (substituteLastIndex) {
                            // If there was an error, don't discard all the old data: keep the last SubIndex unchanged
                            // and start a new one
                            toRemoveAndRecycle = std::make_shared<SubIndex>();
                            toRemoveAndRecycle->data.reserve(maxChunkSize);
                            toRemoveAndRecycle->indexData.reserve(maxChunkSize);
                            substituteLastIndex = false;
                        }
                        // else the error is on completely new data (not mixed with old indexed data) -> just discard it
                    }
                    SPDLOG_LOGGER_DEBUG(logger, "Finished updating suffix array");
                    // Start a new subindex
                    if (indexes.size() >= maxIndexes) {
                        // Remove the first index (recycle the memory to avoid deallocation slowdown)
                        newSubindex = toRemoveAndRecycle;
                        // If we exceed the capacity, the next to be removed is the first one,
                        // but wait to remove it until a new one is ready (later pop_front)
                        toRemoveAndRecycle = indexes.front();
                        // Wait for the newSubindex (which was recycled from the indexes) to not to be used
                        // in any other thread before clearing for recycling
                        while (newSubindex.use_count() > 1) {
                            std::this_thread::sleep_for(std::chrono::milliseconds(5));
                        }
                        newSubindex->data.clear();
                        newSubindex->indexData.clear();
                    } else {
                        // Start a new SubIndex
                        newSubindex = std::make_shared<SubIndex>();
                    }
                    newSubindex->data.reserve(maxChunkSize);
                    newSubindex->indexData.reserve(maxChunkSize);
                    newSubindex->maxSearchEntries = 0;
                    currentChunkSize = 0;
                }

                // Append the sentence
                newSubindex->data.insert(newSubindex->data.end(), sentence.begin(), sentence.end());
                currentChunkSize += sentence.size();
                selfOutputPool->Release(std::move(sentence));
            }
            SPDLOG_LOGGER_DEBUG(logger, "Finished appending sentences");

            // Construct/re-construct suffix array for the last chunk
            if (currentChunkSize > 0) {
                SPDLOG_LOGGER_DEBUG(logger, "Constructing suffix array");
                int res = constructSuffixArrayInplace(newSubindex->data, newSubindex->indexData, vocabSize, logger);
                if (res == 0) {
                    // suffix array construction successful
#if SPDLOG_ACTIVE_LEVEL <= SPDLOG_LEVEL_DEBUG
                    SPDLOG_LOGGER_DEBUG(logger, "Construction successful. Updating indexes.");
                    auto s_time = std::chrono::steady_clock::now();
#endif
                    {
                        // Here I should have already assigned the toRemoveAndRecycle to avoid memory destruction.
                        // We remove indexes later (here) to always use maxIndexes indexes
                        std::lock_guard<std::mutex> lock(indexesMtx);
                        if (substituteLastIndex) {
                            indexes.pop_back();
                        }
                        indexes.push_back(std::move(newSubindex));
                        if (indexes.size() > maxIndexes) {
                            indexes.pop_front();
                        }
                    }
#if SPDLOG_ACTIVE_LEVEL <= SPDLOG_LEVEL_DEBUG
                    auto e_time = std::chrono::steady_clock::now();
                    elapsed_ms = std::chrono::duration_cast<std::chrono::microseconds>(e_time - s_time).count();
                    SPDLOG_LOGGER_DEBUG(logger, "Time taken to update last index: {} us", elapsed_ms);
#endif
                } else {
                    // If there was an error, it is almost impossible that the problem was in the data already indexed,
                    // but to be sure to avoid always discarding (also in the next update) delete the index anyways.
                    SPDLOG_LOGGER_WARN(logger,
                        "The construction of the suffix array returned error {}. The new data will be discarded.",
                        res);
                    {
                        std::lock_guard<std::mutex> lock(indexesMtx);
                        if (substituteLastIndex) {
                            indexes.pop_back();
                        }
                        if (indexes.size() > maxIndexes) {
                            // should not be triggered, this 'if' could be removed
                            indexes.pop_front();
                        }
                    }
                }
                SPDLOG_LOGGER_DEBUG(
                    logger, "Finished constructing the new datastore. Number of indexes: {}", indexes.size());
            }

            // Update the subsampling parameters of the indices
            SetSearchEntriesPerIdx();

            // The new subIndex will recycle the memory of the first to discard: wait (earlier in the loop) that no one
            // has references to it
            newSubindex = toRemoveAndRecycle;
        }
    }
}

/**
 * After changing the indexes, or updating maxSearchEntries, the entries per index
 * must be updated.
 * This is just a helper method to avoid repeating code, not thread-safe.
 */
void Reader::SetSearchEntriesPerIdx() {
    // If you mutate/read indexes from multiple threads, uncomment:
    // std::lock_guard<std::mutex> lock(indexesMtx);

    if (indexes.empty()) return;

    // Sum all elements
    size_t totalElements = 0;
    for (const auto& sp : indexes) {                // sp is std::shared_ptr<SubIndex>
        totalElements += sp->data.size();
    }

    const size_t firstIndexSize = indexes.front() ? indexes.front()->data.size() : 0;

    for (auto& sp : indexes) {
        const size_t idxSize = sp->data.size();

        // Proportional allocation (rounded to nearest)
        const size_t proportional = static_cast<size_t>(std::llround(
            static_cast<double>(maxSearchEntries) *
            (totalElements ? (static_cast<double>(idxSize) / static_cast<double>(totalElements)) : 0.0)
        ));

        // Each index should lookup at least 20 elements (or proportionally less for the last one)
        const size_t minByFirst = (firstIndexSize > 0)
            ? static_cast<size_t>(std::llround(20.0 * static_cast<double>(idxSize) /
                                               static_cast<double>(firstIndexSize)))
            : 0;

        size_t value = std::max(proportional, minByFirst);

        sp->maxSearchEntries.store(value, std::memory_order_relaxed);
    }
}

void Reader::SaveIndexesToDisk(const std::string &path)
{
    // Start a new thread to save the indexes
    std::thread save_thread(&Reader::WriteIndexesToDisk, this, path);
    save_thread.detach();
}

void Reader::WriteIndexesToDisk(const std::string &path)
{
    static std::mutex write_mutex;

    // Try to acquire the lock without waiting
    if (!write_mutex.try_lock()) {
        SPDLOG_LOGGER_WARN(logger,
            "I am still writing the indexes from a previous call! Don't call save indexes so frequently! This call "
            "will be discarded.");
        return;
    }

    // Ensure the mutex is unlocked when we exit the function
    std::lock_guard<std::mutex> lock(write_mutex, std::adopt_lock);

    auto indexes_to_save = indexes;  // Shared pointer copy

    // Open the file (removes the content if present)
    std::ofstream indexFile(path, std::ios::out | std::ios::binary);
    if (!indexFile.is_open()) {
        SPDLOG_LOGGER_WARN(logger, "Unablle to open file {} to write.", path);
        return;
    }

    int32_t flag = 0;  // flag to say that we are using 4 bytes per token
    indexFile.write(reinterpret_cast<char *>(&flag), sizeof(flag));

    // Write the data
    for (const auto &subIndex : indexes_to_save) {
        // Write the data size and data
        uint32_t data_size = subIndex->data.size() * sizeof(int32_t);
        indexFile.write(reinterpret_cast<const char *>(&data_size), sizeof(data_size));
        indexFile.write(reinterpret_cast<const char *>(subIndex->data.data()), data_size);

        // Write the index size and index data
        uint32_t index_size = subIndex->indexData.size() * sizeof(int32_t);
        indexFile.write(reinterpret_cast<const char *>(&index_size), sizeof(index_size));
        indexFile.write(reinterpret_cast<const char *>(subIndex->indexData.data()), index_size);
    }

    indexFile.close();
}

void Reader::PrintIndexes()
/*
Just for debugging on toy data, don't use for real!
*/
{
    size_t indexCounter = 0;
    for (const auto &subIndex : indexes) {
        std::cout << "SubIndex " << indexCounter++ << ":\n";

        // Print data vector
        std::cout << "  Data: ";
        for (const auto &val : subIndex->data) {
            std::cout << val << " ";
        }
        std::cout << "\n";

        // Print indexData vector
        std::cout << "  IndexData: ";
        for (const auto &val : subIndex->indexData) {
            std::cout << val << " ";
        }
        std::cout << "\n";

        // Print maxSearchEntries
        std::cout << "  MaxSearchEntries: " << subIndex->maxSearchEntries.load() << "\n";
    }
}

// UTILS

CompareResult compare_ranges(std::vector<int32_t>::const_iterator start1, std::vector<int32_t>::const_iterator end1,
    std::vector<int32_t>::const_iterator start2, std::vector<int32_t>::const_iterator end2)
{
    if (std::equal(start1, end1, start2, end2)) {
        return CompareResult::EQUAL;
    }
    return std::lexicographical_compare(start1, end1, start2, end2) ? CompareResult::LESS : CompareResult::GREATER;
}
